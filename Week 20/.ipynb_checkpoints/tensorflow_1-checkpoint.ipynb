{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CF969 - Big Data for Computational Finance\n",
    "## Lab 5b: Introduction to TensorFlow\n",
    "\n",
    "TensorFlow is a Python library developed by Google which is ideal for working with neural networks and deep learning.\n",
    "It has the following especially useful features:\n",
    "* Fast tensor and matrix multiplication (using GPU if present)\n",
    "* Built-in automatic symbolic differentiation \n",
    "    * In particular, built-in algorithms for back-propagation \n",
    "* Lots of canned neural network structures are included:\n",
    "    * Feed-forward neural networks,\n",
    "    * Convolutional Networks,\n",
    "    * Recurrent Neural Networks,\n",
    "    * LSTMs.\n",
    "    * (You might not be familiar with some of the above items in the list. We will learn about these in later lectures.)\n",
    "* Lots of canned optimisation algorithms are included\n",
    "    * Stochastic Gradient Descent,\n",
    "    * RMSprop,\n",
    "    * Adam.\n",
    "    * (In the lectures we only cover Stochastic Gradient Descent. The other two I just mentioned are in some sense variations on this, and in general this holds for most optimisation algorithms used for training neural networks.)\n",
    "\n",
    "The present Notebook for this lab is based on a corresponding notebook written by Bart de Keijzer and on a tutorial and a big set of slides created by Dr. Michael Fairbank. Some of the Python code scripts here are based on code from tensorflow.org and from https://github.com/aymericdamien/TensorFlow-Examples/, by Aymeric Damien.\n",
    "\n",
    "When going through these notes, please experiment with the pieces of code, take your time with them, and look up the meaning of the various statements in the online TensorFlow documentation whenever you do not fully understand what is happening. \n",
    "\n",
    "## Basics\n",
    "\n",
    "We start with writing a simple program in TensorFlow that outputs the string \"Hello world!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Hello World!', shape=(), dtype=string)\n",
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "hello = tf.constant(\"Hello World!\")\n",
    "print(hello)\n",
    "print(hello.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow works with tensors as its elementary data object. A tensor is a generalisation of a matrix to higher dimensions. The number of dimensions of a tensor is called the _rank_ of the tensor. Here are some examples.\n",
    "* The rank of [1 2 3 4] is 1;\n",
    "* The rank of [[1 2],[3 4]] is 2 (Here I wrote down a 2 by 2 matrix on a single line);\n",
    "* The rank of 2 (i.e., a scalar) is 0.\n",
    "\n",
    "In TensorFlow the aim should be to build our code as much as possible out of tensors and matrix multiplications. This ultimately allows to \"outsource\" such operations to the GPU of your computer, as GPUs are generally very efficient at executing fast matrix multiplications.\n",
    "\n",
    "(For an explanation of the meaning of the 'b' in front of the output, see: https://stackoverflow.com/questions/6269765/what-does-the-b-character-do-in-front-of-a-string-literal)\n",
    "\n",
    "Let's go over some simple example code and analyse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant(2)\n",
    "b=tf.constant(3)\n",
    "c=tf.add(a,b)\n",
    "print(c)\n",
    "print(c.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two lines define two tensors of rank 0, i.e., two _scalars_. The next line adds them together to form the rank 0 tensor named _c_. The final two lines print the content of c. Observe that if we care only about the numerical value of c, we must call c.numpy()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with 2D tensors, i.e., matrices. Addition of tensors in TensorFlow is defined element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1,2],[3,4]])\n",
    "b=tf.constant([[5,6],[8,9]])\n",
    "c=tf.add(a,b)\n",
    "print(c)\n",
    "print(c.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the above syntax for specifying a 2D tensor. You should be able to infer from this example how to define a 3D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define two 3D tensors of the same shape and add them together.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function _multiply_ in tensorflow performs element-wise multiplication on tensors. This operations is also referred to as the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)). Note that this is _not_ the same as matrix multiplication when performed on 2D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1,2],[3,4]])\n",
    "b=tf.constant([[5,6],[8,9]])\n",
    "c=tf.multiply(a,b)\n",
    "print(c)\n",
    "print(c.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function _matmul_ performs matrix multiplication. It works whenever the number of columns of the first 2D tensor equals the number of rows of the second 2D tensor. In case we only care directly about the numerical result, we can use np.matmul instead of tf.matmul (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1.5,2],[3,4]], tf.float32)\n",
    "b=tf.constant([[1],[2.5]], tf.float32)\n",
    "c=tf.matmul(a,b)\n",
    "print(c)\n",
    "print(c.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1.5,2],[3,4]], tf.float32)\n",
    "b=tf.constant([[1],[2.5]], tf.float32)\n",
    "c=np.matmul(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types and Casting\n",
    "\n",
    "When you create a tensor, it is good practice to specify its datatype, in our above code we did so in some cases (where we specified *tf.float32*), but there were also cases where we omitted this. Omitting it will make TensorFlow default to using the _int32_ (i.e., a 32 bit integer number) or _float32_ datatype, depending on whether you write a number with a decimal point included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant(3.2, tf.float32)\n",
    "b=tf.constant(3, tf.int32)\n",
    "c=tf.constant([1,2,3], tf.float32)\n",
    "d=tf.constant(5)\n",
    "e=tf.constant(5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first three lines of the code above, we specified the datatype. In the fourth line, d has type _int32_ and in the fifth line, e has type _float32_. There are other data types, such as *float64*, *int64*, and *bool*. \n",
    "Data types can be converted to each other using _cast_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1.0,2.0],[3.0,-4.0]],tf.float32)\n",
    "print(tf.cast(a,tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output matrix is indeed an _int32_ matrix, otherwise the decimal points would be shown in the output. Casting floats to ints will result in rounding down the fractional parts of the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1.7,2.3],[3.0,-4.0]],tf.float32)\n",
    "print(tf.cast(a,tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting bools to ints will result in conversion of _true_ values to 1s, and _false_ values to 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=tf.constant([True, False, True], tf.bool)\n",
    "print(tf.cast(b,tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If datatypes do not match, many basic operations will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant(3.0, tf.float32)\n",
    "b=tf.constant(3, tf.int32)\n",
    "c=tf.add(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do the above successfully, we should use _cast_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=tf.add(a,tf.cast(b,tf.float32))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapes, Broadcasting, and Shorthands\n",
    "\n",
    "The shape of a tensor denotes the rank $r$ and the number of entries in each of the $r$ dimensions. For many basic operations on two tensors, the shapes must match. (But not for all, such as for _matmul_. For _matmul_ there are other constraints on the shape of the operands.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([1,2])\n",
    "b=tf.constant([2,3,1])\n",
    "f=tf.constant(tf.add(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, TensorFlow makes exceptions to the requirement of having matching shapes. For example, when the rank of one tensor is less than the rank of the other, TensorFlow will apply various rules to implicitly transform the smaller-rank tensor to the shape of the higher-rank tensor in such a way that the operation is performed in the most natural way possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([1,2])\n",
    "b=tf.constant(1)\n",
    "print(tf.add(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1,2],[3,4]])\n",
    "b=tf.constant([10,20])\n",
    "print(tf.add(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first of these two examples, the rank 0 tensor 1 is implicitly converted to [1,1] before TensorFlow performs addition. In the second example, the rank 1 tensor [10,20] is implicitly converted to [[10,20],[10,20]]. \n",
    "\n",
    "This behaviour is called _broadcasting_. This can be convenient but also very confusing. A precise description of broadcasting rules can be found at https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html .\n",
    "\n",
    "We have seen up to now various operations on tensors that take two operands. There are also operations that take a single operand. In the below examples we see how to compute respectively the element-wise square, absolute value, and hyperbolic tangent of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1,2],[3,-4]],tf.float32)\n",
    "print(tf.square(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.abs(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.tanh(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform element-wise comparison. See for example https://www.tensorflow.org/api_docs/python/tf/math/greater. These operations result in tensors of *bools*: one for each element, indicating if the respective comparison is true for each of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([1,2,3])\n",
    "b=tf.constant([5,1,7])\n",
    "print(tf.greater(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that broadcasting will also work in those cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.greater(a,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of TensorFlow's functions have shorthand notation: One can write * for *multiply*, + for *add*, - for *subtract*, and > for *greater*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1,2],[3,4]])\n",
    "b=tf.constant([[5,6],[8,9]])\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "TensorFlow _variables_ are tensors and are the recommended way to represent shared, persistent state your program manipulates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[5,2],[1,2]])\n",
    "var_a = tf.Variable(a)\n",
    "b=tf.add(var_a,var_a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable looks and acts like a tensor, and, in fact, is a data structure backed by a tf.Tensor. Like tensors, they have a dtype and a shape, and can be exported to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \", var_a.shape)\n",
    "print(\"DType: \", var_a.dtype)\n",
    "print(\"As NumPy: \", var_a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables can be updated by using the _assign_ function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable([0.3], tf.float32)\n",
    "W.assign([-1.0])\n",
    "print(W.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however that it is not allowed to resize the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  W.assign([1.0, 2.0, 3.0])\n",
    "except Exception as e:\n",
    "  print(f\"{type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "If you use a variable like a tensor in operations, you will usually operate on the backing tensor. Creating new variables from existing variables duplicates the backing tensors. Two variables will not share the same memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable([2.0, 3.0])\n",
    "# Create b based on the value of a\n",
    "b = tf.Variable(a)\n",
    "a.assign([5, 6])\n",
    "\n",
    "# a and b are different\n",
    "print('a= ',a.numpy())\n",
    "print('b= ',b.numpy())\n",
    "\n",
    "# There are other versions of assign\n",
    "print(a.assign_add([2,3]).numpy())  # [7. 9.]\n",
    "print(a.assign_sub([7,9]).numpy())  # [0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Functions\n",
    "\n",
    "TensorFlow has various aggregation functions, most of them starting with the word _reduce_. The following examples compute respectively the sum, mean, and maximum of the elements of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1,2],[3,4]],tf.float32)\n",
    "print(tf.reduce_sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.reduce_mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.reduce_max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Try to predict what will be the result of the following code. Run the code to verify your answer.\n",
    "\n",
    "a=tf.constant([[1,2],[3,4]],tf.float32)\n",
    "print(tf.reduce_sum(tf.cast(a>1,tf.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function _argmax_ takes a tensor and returns the index where the maximum value among the entries in the tensor occurs. Note here that indexing starts at 0. Observe the difference between _reduce&#95;max_ and *argmax*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([4,0,5,-4],tf.float32)\n",
    "print(tf.reduce_max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.argmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([4,0,5,-4],tf.float32)\n",
    "print([tf.reduce_min(a),tf.reduce_max(a)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation functions can be restricted to be evaluated over a single axis of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[1,2],[3,4]])\n",
    "print(tf.reduce_sum(a, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.reduce_sum(a, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Based on the previous examples, you can predict how axis numbering works on tensors of rank higher than 2. \n",
    "# What would be the result of the following code? Run the code to verify your answer.\n",
    "\n",
    "a=tf.constant([[[5,6],[7,8]],[[1,2],[3,4]]])\n",
    "print(tf.reduce_sum(a, axis=2))\n",
    "print(tf.reduce_sum(a, axis=1))\n",
    "print(tf.reduce_sum(a, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done with other aggregation functions, such as _reduce&#95;max_ and *argmax*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[5,10,0],[3,4,12]])\n",
    "print(tf.reduce_max(a, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.argmax(a, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "TensorFlow is capable of symbolically differentiating mathematical formulas (expressed in TensorFlow's syntax). Consider the following example. Recall that ** is the operator corresponding to exponentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.Variable(5.0,tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, that are usually tf.Variables. TensorFlow \"records\" relevant operations executed inside the context of a tf.GradientTape onto a \"tape\" and uses that tape to compute the gradients.\n",
    "\n",
    "The above piece of code first defines a variable x and sets its value to $5$, then defines $y$ to stand for the multiplication of $x$ with itself, i.e., $y = x^2$. It defines dy_dx to be the derivative of y with respect to x. This is done by calling the function _gradient_ and specifying as its first argument the mathematical function to differentiate, and as its second argument a list of variables with respect to which the function is differentiated. In this case, we differentiate with respect to only the variable x. If more variables are specified, dydx will be a (Python) list containing the respective derivatives as its element. \n",
    "\n",
    "The following is an example of a two-variable function, of which we can compute both its derivatives. Observe the two different ways of doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.Variable(4.0,tf.float32)\n",
    "y=tf.Variable(2.0,tf.float32)\n",
    "\n",
    "# First way\n",
    "with tf.GradientTape() as tape:\n",
    "    f=tf.pow(x,2.0)*3.0+y\n",
    "z = tape.gradient(f,{'x': x,'y': y})\n",
    "print('dz/dx:', z['x'])\n",
    "print('dz/dy:', z['y'])\n",
    "\n",
    "# Second way: we get the gradients as a list\n",
    "with tf.GradientTape() as tape2:\n",
    "    ff=tf.pow(x,2.0)*3.0+y\n",
    "[dfdx, dfdy] = tape2.gradient(ff,[x,y])\n",
    "print(dfdx)\n",
    "print(dfdy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining both ways using two tapes\n",
    "x=tf.Variable(4.0,tf.float32)\n",
    "y=tf.Variable(2.0,tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\n",
    "    f=tf.pow(x,2.0)*3.0+y\n",
    "z = tape0.gradient(f,{'x': x,'y': y})\n",
    "[dfdx,dfdy] = tape1.gradient(f,[x,y])\n",
    "print('dz/dx:', z['x'])\n",
    "print('dz/dy:', z['y'])\n",
    "print(dfdx)\n",
    "print(dfdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation can also handle variables that are tensors of higher rank than 0. This feature is very useful to neural network programming, and makes it very easy to implement backpropagation algorithms, which are fundamental to training neural networks (we will discuss backpropagation at some point in the lectures).\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "_Gradient descent_ stands for a family of algorithms to find minima (or maxima) of functions. Suppose that we want to find the minimum of some function f(x). A basic backpropagation algorithm then works as follows. Fix a parameter _eta_ to any positive small value. Start with any initial input x_0 to the function. Compute the derivative _dfdx_ of f at x_0 and set x_1 to the value x_0 - _eta_ \\* _dfdx_. If f(x_1) < f(x_0), then we repeat this step on x_1, resulting in a sequence of points x_0, x_1, x_2, ..., x_k, where for x_k it holds that an additional iteration performed on x_k would not yield a decrease in the value of f.\n",
    "\n",
    "![title](images/gradientdescent.png)\n",
    "\n",
    "For this algorithm, it is clearly important to not make $\\eta$ (*eta*) (which is often referred to as the *step size* or *learning rate*) too large, as this might result in the function not having decreased at the next iteration while a decrease could have been possible if $\\eta$ were smaller. It is also important to not make $\\eta$ too small, as this might result in very tiny progress each step. Please study the following code and complete the three lines containing \"TODO\" comments. It should  use gradient descent to minimise the function $f(x) = x^2 - 4x + 4$. \n",
    "\n",
    "![title](images/function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(10.0, tf.float32) # arbitrary initial value\n",
    "eta = 0.1 # learning rate\n",
    "\n",
    "for i in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y =                    #TODO put in formula for y = x^2 - 4x + 4\n",
    "\n",
    "    dydx=tape.gradient(        # TODO finish this line\n",
    "    x.assign(                  # TODO finish x_(t+1)=x_t-eta*dydx.\n",
    "    print(\"iteration:\",i,\"x:\", x.numpy(),\" y:\",y.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature of TensorFlow is that it has many optimisers built in, and one of these optimisers is the basic gradient descent optimiser. It can be accessed under _optimizers.SGD_ and takes the learning rate $\\eta$ as a parameter. Therefore, the following code does the same as your solution above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(10.0, dtype=tf.float32) # arbitrary initial value\n",
    "eta = 0.1 # learning rate\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=eta)    \n",
    "loss = lambda:x**2-4.0*x+4.0\n",
    "\n",
    "for i in range(50):\n",
    "    optimizer.minimize(loss,[x])\n",
    "    print(\"iteration:\",i, \"x:\", x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have specified through _optimizer.minimize()_ which function the built-in Stochastic Gradient Descent Optimizer should minimize. TensorFlow then automatically infers on which variables y depends.\n",
    "\n",
    "There are various other canned optimisers in TensorFlow, such as the _Adam_ and _RMSprop_. As an exercise, please modify the above code and run it with those two optimisers instead of the _SGD_ optimizer.\n",
    "\n",
    "Gradient descent, and various derived optimisation methods are used heavily in neural networks and algorithms for deep learning, to train deep neural networks. You probably understand from the above explanation and code that neural networks makes \"local\" improvements, and can end up in a local minimum of a function. This is why gradient descent is often applied multiple times on a given function, at multiple starting positions, in the hope to find the global minimum among the set of local minima that a function may have.\n",
    "\n",
    "What we have seen here is gradient descent applied on a function of a single variable. Gradient descent can also be applied on functions of many variables, which is how it is applied when one has to train neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Linear Regression\n",
    "We now see an example of linear regression. Linear regression is perhaps the simplest example of a Machine Learning algorithm. In fact, many would argue that linear regression is a fundamental notion, well-known for example in Statistics, and that renaming it as a Machine Learning algorithm is more of a *marketing* thing.\n",
    "\n",
    "We are given in input a set of pairs of the form $(x,y)$. Our goal is to find a lienar function $f()$ that fits the pairs $(x,y)$ in the best possible way. We will talk more about it in the next lectures.\n",
    "\n",
    "We assume, therefore, that there exist parameters $W$ and $b$ such that $f(x) = W\\cdot x+b$. $W$ is also known as the slope, while $b$ is known as the intercept. \n",
    "\n",
    "We view each pair $(x,y)$ as an observation that originates from this function. Inevitably, our function $f()$ will not fit all pairs, i.e., there will be errors since we restrict to be linear. Our goal is to minimize this error.\n",
    "\n",
    "Observe that below we follow a modular approach. That is, we define appropriate functions and use them where appopriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rng = np.random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters.\n",
    "learning_rate = 0.01\n",
    "training_steps = 1000\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data.\n",
    "X = np.array([2.167, 3.1, 3.3, 4.168, 4.4, 5.313, 5.5, 5.654, 6.182, 6.71, 6.93, 7.042, 7.59, 7.997, 9.27, 9.779, 10.791])\n",
    "Y = np.array([1.221,  1.3, 1.573, 1.65, 1.694, 1.7,2.09, 2.42, 2.53, 2.596, 2.76, 2.827, 2.904, 2.94, 3.19, 3.366, 3.465])\n",
    "\n",
    "# Let's plot the data. We will plot them again later.\n",
    "plt.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight and Bias, initialized randomly.\n",
    "slope = tf.Variable(rng.randn(), name=\"slope\")\n",
    "intercept = tf.Variable(rng.randn(), name=\"intercept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression (Wx + b). This is where the define the corresponding function.\n",
    "def linear_regression(x):\n",
    "    return slope* x + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean square error. This is where we define the error function.\n",
    "# For each data point, we compute (predicted_value - true_value)^2 and sum over all data points\n",
    "def mean_square(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent Optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. \n",
    "def run_optimization():\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = linear_regression(X)\n",
    "        loss = mean_square(pred, Y)\n",
    "\n",
    "    # Compute gradients. Since we have two variables, we get two gradients.\n",
    "    gradients = g.gradient(loss, [slope, intercept])\n",
    "    \n",
    "    # Update W and b following gradients. What does zip do? Can you figure it out?\n",
    "    optimizer.apply_gradients(zip(gradients, [slope, intercept]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training for the given number of steps.\n",
    "for step in range(1, training_steps + 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization()\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = linear_regression(X)\n",
    "        loss = mean_square(pred, Y)\n",
    "        print(\"step: %i, loss: %f, W: %f, b: %f\" % (step, loss, slope.numpy(), intercept.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the solution\n",
    "\n",
    "xfit = np.linspace(0, max(X))\n",
    "yfit = slope*xfit+intercept\n",
    "plt.scatter(X,Y)\n",
    "plt.plot(xfit,yfit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free the play with the data points and see how this changes the output. You can also play with the learning rate and the number of steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
